import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import glob
import os
from tqdm import tqdm
from networks import TinyEncoder, Predictor

# --- Configuration ---
BATCH_SIZE = 256
EPOCHS = 50
LR = 1e-4
DATA_PATH = "./data/*.npz"
# This will be the file generated by your current tmux session
ENCODER_PATH = "./models/encoder_final.pth" 
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- Dataset (Triplets) ---
class DynamicsDataset(Dataset):
    def __init__(self, file_pattern):
        self.files = glob.glob(file_pattern)
        self.obs = []
        self.actions = []
        self.next_obs = []
        
        print(f"Loading files for dynamics...")
        for f in self.files:
            try:
                with np.load(f) as arr:
                    # Check keys
                    o = arr['obs'] if 'obs' in arr else arr['arr_0']
                    a = arr['action'] if 'action' in arr else arr['arr_1']
                    
                    # Create triplets: State(t), Action(t), State(t+1)
                    # We strip the last frame from obs, and the first frame from next_obs
                    self.obs.append(o[:-1])
                    self.actions.append(a[:-1])
                    self.next_obs.append(o[1:])
            except Exception as e:
                # Silently skip bad files to avoid crashing
                pass
                
        self.obs = np.concatenate(self.obs, axis=0)
        self.actions = np.concatenate(self.actions, axis=0)
        self.next_obs = np.concatenate(self.next_obs, axis=0)
        
        # NHWC -> NCHW
        self.obs = np.transpose(self.obs, (0, 3, 1, 2))
        self.next_obs = np.transpose(self.next_obs, (0, 3, 1, 2))
        
        print(f"Total triplets: {len(self.obs)}")

    def __len__(self):
        return len(self.obs)

    def __getitem__(self, idx):
        obs = torch.from_numpy(self.obs[idx]).float() / 255.0
        action = torch.from_numpy(self.actions[idx]).float()
        next_obs = torch.from_numpy(self.next_obs[idx]).float() / 255.0
        return obs, action, next_obs

# --- Training ---
def train():
    print(f"Training Predictor on {DEVICE}")
    
    # 1. Setup Models
    encoder = TinyEncoder().to(DEVICE)
    predictor = Predictor(input_dim=512, action_dim=3).to(DEVICE)
    
    # 2. Load and Freeze Encoder
    if os.path.exists(ENCODER_PATH):
        print(f"Loading Frozen Eye from {ENCODER_PATH}")
        encoder.load_state_dict(torch.load(ENCODER_PATH))
    else:
        print("WARNING: Encoder weights not found! (Are you running this before Day 2 finishes?)")
    
    encoder.eval()
    for param in encoder.parameters():
        param.requires_grad = False

    # 3. Optimization
    optimizer = optim.Adam(predictor.parameters(), lr=LR)
    criterion = nn.MSELoss() # We want Predicted Latent == Actual Latent
    scaler = torch.amp.GradScaler('cuda')

    # 4. Data
    dataset = DynamicsDataset(DATA_PATH)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True)

    os.makedirs("models", exist_ok=True)

    for epoch in range(EPOCHS):
        predictor.train()
        total_loss = 0
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{EPOCHS}")

        for obs, action, next_obs in pbar:
            obs, action, next_obs = obs.to(DEVICE, non_blocking=True), action.to(DEVICE, non_blocking=True), next_obs.to(DEVICE, non_blocking=True)

            optimizer.zero_grad()

            with torch.amp.autocast('cuda'):
                # Get Ground Truth Latents (Using the Frozen Eye)
                with torch.no_grad():
                    z_t = encoder(obs)
                    z_t1_true = encoder(next_obs)

                # Predict the future
                z_t1_pred = predictor(z_t, action)

                # Loss: Minimize distance between "Dream" and "Reality"
                loss = criterion(z_t1_pred, z_t1_true)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss.item()
            pbar.set_postfix(loss=f"{loss.item():.6f}")

        # Save Checkpoint
        if (epoch+1) % 10 == 0:
            torch.save(predictor.state_dict(), f"models/predictor_ep{epoch+1}.pth")

    torch.save(predictor.state_dict(), "models/predictor_final.pth")
    print("Predictor Training Complete.")

if __name__ == "__main__":
    train()