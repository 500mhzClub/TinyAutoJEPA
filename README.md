# TinyAutoJEPA: Self-Supervised World Model Pilot

TinyAutoJEPA is an autonomous driving system based on Joint Embedding Predictive Architecture (JEPA). It focuses on learning the dynamics of an environment through self-supervised observation rather than direct imitation.

Unlike standard Imitation Learning, which clones human actions directly, this system builds a "World Model" (an internal representation of physics and causality). It learns to distinguish surface properties (e.g., grass vs. road) and predicts future states based on current actions before being tasked with driving.

## Hardware Configuration

This project is configured for a high-performance AMD workstation running Linux. It utilizes a dual-GPU setup to parallelize feature learning (Encoder) and visual verification (Decoder).

| Component | Specification | Function |
| --- | --- | --- |
| **CPU** | **AMD Ryzen 9 5950X** (16C/32T) | Handles massive parallel data generation and loading. Runs 32 concurrent Gym environments during data collection and manages dataset shuffling for the ~5.8 million frame buffer. |
| **GPU 1** | **AMD Radeon RX 6950 XT** (16GB) | Primary compute unit. Dedicated to training the **Encoder**, **Predictor**, and **Cost Model**. Leverages ROCm mixed-precision (`torch.amp`) to handle large batch sizes (256) and VICReg loss calculations. |
| **GPU 2** | **9060 XT** | Secondary compute unit. Runs the **Parallel Decoder** in the background. It dynamically loads checkpoints from GPU 1 to visualize the latent space in real time without pausing the main training loop. |
| **RAM** | **64 GB DDR4** | Facilitates OS-level caching of the dataset, minimizing disk I/O latency during repeated training epochs. |
| **OS** | **Pop!_OS (Linux)** | Selected for native ROCm kernel support and efficient thread scheduling. |

## Dataset Strategy

The system relies on a massive, multi-modal dataset to learn a robust world model. The data is sourced from three distinct collection strategies to ensure a rich understanding of different driving conditions, preventing issues like "Catastrophic Forgetting" (e.g., forgetting grass physics because the car only drives on the track).

*   **Total Volume:** ~5.8 million frames.
*   **Generation:** Produced via 32 parallel CPU workers running different collection scripts.
*   **Composition:**
    *   **Expert Driving (~1.9M frames):** Generated by a pure-pursuit algorithm that follows racing lines perfectly. This teaches the model the ideal driving behavior ("Order").
    *   **Recovery Scenarios (~2.4M frames):** Generated by an expert policy that has intentional mistakes injected (e.g., understeering, oversteering, late braking). This forces the model to learn how to recover from errors and return to a stable state.
    *   **Random Exploration (~1.5M frames):** Generated by taking random actions to explore the full range of the environment, including off-road areas and unusual orientations ("Chaos").
*   **Balancing:** The data loader in the training scripts ensures a balanced mix of these data types in each batch, allowing the model to learn from all modalities simultaneously.

## Training Pipeline

### Step 1: Mass Data Generation

Generates trajectory data using the 32-thread collection scripts. This saturates the CPU to maximize throughput. The three scripts can be run in any order to generate the full dataset.

```bash
# Generate expert driving data
python collect_race_data.py
# Output: ./data_race/*.npz (~1.9M frames)

# Generate recovery scenario data
python collect_recovery_data.py
# Output: ./data_recovery/*.npz (~2.4M frames)

# Generate random exploration data
python collect_data.py
# Output: ./data/*.npz (~1.5M frames)
```

### Step 2: Encoder Training (The Foundation)

Trains the visual encoder on the primary GPU. This step uses a **VICReg** loss function to force the model to learn distinct features for different track elements without requiring labeled data.

```bash
python train_encoder.py
# Output: models/encoder_mixed_ep*.pth

```

### Step 3: Parallel Visual Verification

Runs simultaneously on the secondary GPU. This script hunts for new checkpoints generated by Step 2, freezes the encoder, and attempts to reconstruct images. This allows for real-time monitoring of the "World Model's" visual acuity (e.g., watching rumble strips appear in the reconstructions as the encoder learns).

```bash
python train_decoder.py
# Output: visuals/reconstruct_ep*.png

```

Heres an example, though features are still sharpening up:

<img src="reconstruct_ep43.png" width="800" alt="Reconstruction Epoch 43">

*Note: This is a test of the encoder to verify it is successfully extracting features and embedding them into a vector that can be reconstructed.*

### Step 4: Multi-Step Predictor Training

Trains the latent dynamics model. Unlike previous iterations, this uses a **Multi-Step Horizon (T+15)**. The model must predict a chain reaction of 15 future states. This forces it to learn causal physics rather than just copying the previous frame.

```bash
python train_predictor.py

```
Initial Dream visualisation, predictor generating possible futures: 

![Dream Visualization](output_dream.gif)

### Step 5: Cost Model Training (The Judge)

Trains the energy-based discriminator. This model learns to distinguish "Expert" behavior (Low Energy) from "Random" behavior (High Energy) in the latent space. It serves as the navigation compass for the MPC.

```bash
python train_cost_model.py
# Output: models/cost_model_final.pth

```

## Component Architecture

### 1. The Encoder (`TinyEncoder`)

**Layman Explanation:**
The Encoder functions as the system's vision. It compresses raw, high-dimensional image data (pixels) into a compact summary vector. This vector allows the system to process the scene's content (e.g., "curve ahead") without processing every individual pixel.

**Technical Detail:**
The architecture is a modified **ResNet18** backbone. Max-pooling layers have been removed to preserve spatial granularity essential for the 64x64 input resolution. The training process uses a balanced loader to ensure the latent space encodes both track geometry and off-road textures.

### 2. The Multi-Step Predictor (`Predictor`)

**Layman Explanation:**
The Predictor functions as the system's internal simulator. It takes the current state summary and a proposed sequence of actions (e.g., "accelerate, then steer left") to calculate the likely outcome 15 steps into the future.

**Technical Detail:**
This is an autoregressive Multi-Layer Perceptron (MLP).

* **Input:** Current Latent State () and Action ().
* **Mechanism:** The output () is fed back into the input to predict , , etc.
* **Objective:** Minimize the Mean Squared Error (MSE) over the entire prediction horizon ( to ). This prevents "lazy" predictions where the model assumes the state is static.

### 3. The Decoder (`TinyDecoder`)

**Layman Explanation:**
The Decoder translates the internal numerical summaries back into visible images. It is primarily a debugging tool for humans to verify that the Encoder is actually "seeing" the road markers and grass boundaries correctly.

**Technical Detail:**
The architecture is a **Transposed Convolutional Network** (Inverse ResNet). It projects the 512-dimensional latent vector into a 4x4 spatial feature map and progressively upsamples it to the original 64x64 resolution using learned convolutional filters.

### 4. The Cost Model (`CostModel`)

**Layman Explanation:**
The Cost Model functions as the system's subconscious "gut feeling." When the Predictor imagines a future where the car is on the grass, the Cost Model assigns it a "High Energy" (Bad) score. When it imagines a future on the racing line, it assigns a "Low Energy" (Good) score.

**Technical Detail:**
A lightweight MLP Classifier trained on the frozen latent embeddings. It outputs a scalar probability score , where  represents the "Expert" distribution and  represents the "Random/Chaos" distribution.

## Energy-Based Autonomous Driving (MPC)

The final phase implements **Model Predictive Control (MPC)** using an Energy-Based Model (EBM) approach. Instead of targeting a specific geometric point, the car minimizes "Energy" in the latent landscape.

**Execution:**

```bash
python drive_mpc.py

```

**MPC Workflow:**

1. **Observation:** Encode the current camera frame into Latent State .
2. **Dreaming:** Generate 1,000 parallel action sequences using **Colored Noise** (temporally correlated random smoothing) to ensure drivable trajectories.
3. **Prediction:** The Predictor unrolls the future latent states () for all 1,000 futures simultaneously.
4. **Judgment:** The Cost Model scores every predicted future state.
* *Grass/Wall*  High Energy.
* *Track/Racing Line*  Low Energy.


5. **Action:** The system executes the first action of the sequence with the lowest cumulative energy.