# TinyAutoJEPA: Self-Supervised World Model Pilot

TinyAutoJEPA is an autonomous driving system based on Joint Embedding Predictive Architecture (JEPA). It focuses on learning the dynamics of an environment through self-supervised observation rather than direct imitation.

Unlike standard Imitation Learning, which clones human actions directly, this system builds a "World Model" (an internal representation of physics and causality). It learns to distinguish surface properties (e.g., grass vs. road) and predicts future states based on current actions before being tasked with driving.

## Hardware Configuration

This project is configured for a high-performance AMD workstation running Linux. It utilizes a dual-GPU setup to parallelize feature learning (Encoder) and visual verification (Decoder).

| Component | Specification | Function |
| :--- | :--- | :--- |
| **CPU** | **AMD Ryzen 9 5950X** (16C/32T) | Handles massive parallel data generation and loading. Runs 32 concurrent Gym environments during data collection and manages dataset shuffling for the 1.9M frame buffer. |
| **GPU 1** | **AMD Radeon RX 6950 XT** (16GB) | Primary compute unit. Dedicated to training the **Encoder** and **Predictor**. Leverages ROCm mixed-precision (`torch.amp`) to handle large batch sizes (256) and VICReg loss calculations. |
| **GPU 2** | **9060 XT** | Secondary compute unit. Runs the **Parallel Decoder** in the background. It dynamically loads checkpoints from GPU 1 to visualize the latent space in real time without pausing the main training loop. |
| **RAM** | **64 GB DDR4** | Facilitates OS-level caching of the dataset, minimizing disk I/O latency during repeated training epochs. |
| **OS** | **Pop!_OS (Linux)** | Selected for native ROCm kernel support and efficient thread scheduling. |

## Dataset Strategy

The system relies on a massive, mixed-modality dataset to prevent "Catastrophic Forgetting" (e.g., forgetting how grass physics work because the car drives perfectly on the track).

* **Total Volume:** ~1,920,000 frames.
* **Generation:** Produced via 32 parallel CPU workers simulating ~640 full racing laps.
* **Composition:** 50% Random Exploration (Off-road recovery, grass physics) and 50% Expert Driving (Lane keeping, racing lines).
* **Balancing:** The data loader actively downsamples the majority class to ensure every training batch contains an equal split of "Order" (Race) and "Chaos" (Random).

## Training Pipeline

### Step 1: Mass Data Generation

Generates trajectory data using the 32-thread collection script. This saturates the CPU to maximize throughput.

```bash
python collect_race_data.py
# Output: ./data_race/*.npz (~1.9M frames total)

```

### Step 2: Encoder Training (The Foundation)

Trains the visual encoder on the primary GPU. This step uses a **VICReg** loss function to force the model to learn distinct features for different track elements without requiring labeled data.

```bash
python train_encoder.py
# Output: models/encoder_mixed_ep*.pth

```

### Step 3: Parallel Visual Verification

Runs simultaneously on the secondary GPU. This script hunts for new checkpoints generated by Step 2, freezes the encoder, and attempts to reconstruct images. This allows for real-time monitoring of the "World Model's" visual acuity (e.g., watching rumble strips appear in the reconstructions as the encoder learns).

```bash
python train_decoder_parallel.py
# Output: visuals/reconstruct_ep*.png

```

### Step 4: Multi-Step Predictor Training

Trains the latent dynamics model. Unlike previous iterations, this uses a **Multi-Step Horizon (T+5)**. The model must predict a chain reaction of 5 future states. This forces it to learn causal physics rather than just copying the previous frame.

```bash
python train_predictor_multistep.py

```

## Component Architecture

### 1. The Encoder (`TinyEncoder`)

**Layman Explanation:**
The Encoder functions as the system's vision. It compresses raw, high-dimensional image data (pixels) into a compact summary vector. This vector allows the system to process the scene's content (e.g., "curve ahead") without processing every individual pixel.

**Technical Detail:**
The architecture is a modified **ResNet18** backbone. Max-pooling layers have been removed to preserve spatial granularity essential for the 64x64 input resolution. The training process uses a balanced loader to ensure the latent space encodes both track geometry and off-road textures.

### 2. The Multi-Step Predictor (`Predictor`)

**Layman Explanation:**
The Predictor functions as the system's internal simulator. It takes the current state summary and a proposed sequence of actions (e.g., "accelerate, then steer left") to calculate the likely outcome 5 steps into the future.

**Technical Detail:**
This is an autoregressive Multi-Layer Perceptron (MLP).

* **Input:** Current Latent State () and Action ().
* **Mechanism:** The output () is fed back into the input to predict , , etc.
* **Objective:** Minimize the Mean Squared Error (MSE) over the entire prediction horizon ( to ). This prevents "lazy" predictions where the model assumes the state is static.

### 3. The Decoder (`TinyDecoder`)

**Layman Explanation:**
The Decoder translates the internal numerical summaries back into visible images. It is primarily a debugging tool for humans to verify that the Encoder is actually "seeing" the road markers and grass boundaries correctly.

**Technical Detail:**
The architecture is a **Transposed Convolutional Network** (Inverse ResNet). It projects the 512-dimensional latent vector into a 4x4 spatial feature map and progressively upsamples it to the original 64x64 resolution using learned convolutional filters.

## Next Steps (Model Predictive Control)

The current system is a passive observer. The next phase involves implementing Model Predictive Control (MPC) to enable active driving.

**MPC Workflow:**

1. **Simulation:** The model generates multiple random action sequences.
2. **Prediction:** The Predictor estimates the future latent states for each sequence over the 5-step horizon.
3. **Evaluation:** A cost function scores the predicted states in latent space (e.g., distance from the "center of track" latent cluster).
4. **Execution:** The action sequence with the optimal score is executed.
