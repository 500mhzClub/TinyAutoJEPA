# TinyAutoJEPA: Self-Supervised World Model Pilot

TinyAutoJEPA is an autonomous driving system based on Joint Embedding Predictive Architecture (JEPA). It focuses on learning the dynamics of an environment through self-supervised observation rather than direct imitation.

Unlike standard Imitation Learning, which clones human actions directly, this system builds a "World Model" (an internal representation of physics and causality). It learns to distinguish surface properties (e.g., grass vs. road) and predicts future states based on current actions before being tasked with driving.

## Hardware Configuration

This project is configured for a high-performance AMD workstation running Linux. It utilizes a dual-GPU setup to parallelize feature learning (Encoder) and visual verification (Decoder).

| Component | Specification | Function |
| --- | --- | --- |
| **CPU** | **AMD Ryzen 9 5950X** (16C/32T) | Handles massive parallel data generation and loading. Runs 32 concurrent Gym environments during data collection and manages dataset shuffling for the 1.9M frame buffer. |
| **GPU 1** | **AMD Radeon RX 6950 XT** (16GB) | Primary compute unit. Dedicated to training the **Encoder**, **Predictor**, and **Cost Model**. Leverages ROCm mixed-precision (`torch.amp`) to handle large batch sizes (256) and VICReg loss calculations. |
| **GPU 2** | **9060 XT** | Secondary compute unit. Runs the **Parallel Decoder** in the background. It dynamically loads checkpoints from GPU 1 to visualize the latent space in real time without pausing the main training loop. |
| **RAM** | **64 GB DDR4** | Facilitates OS-level caching of the dataset, minimizing disk I/O latency during repeated training epochs. |
| **OS** | **Pop!_OS (Linux)** | Selected for native ROCm kernel support and efficient thread scheduling. |

## Dataset Strategy

The system relies on a massive, mixed-modality dataset to prevent "Catastrophic Forgetting" (e.g., forgetting how grass physics work because the car drives perfectly on the track).

* **Total Volume:** ~1,920,000 frames.
* **Generation:** Produced via 32 parallel CPU workers simulating ~640 full racing laps.
* **Composition:** 50% Random Exploration (Off-road recovery, grass physics) and 50% Expert Driving (Lane keeping, racing lines).
* **Balancing:** The data loader actively downsamples the majority class to ensure every training batch contains an equal split of "Order" (Race) and "Chaos" (Random).

## Training Pipeline

### Step 1: Mass Data Generation

Generates trajectory data using the 32-thread collection script. This saturates the CPU to maximize throughput.

```bash
python collect_race_data.py
# Output: ./data_race/*.npz (~1.9M frames total)

```

### Step 2: Encoder Training (The Foundation)

Trains the visual encoder on the primary GPU. This step uses a **VICReg** loss function to force the model to learn distinct features for different track elements without requiring labeled data.

```bash
python train_encoder.py
# Output: models/encoder_mixed_ep*.pth

```

### Step 3: Parallel Visual Verification

Runs simultaneously on the secondary GPU. This script hunts for new checkpoints generated by Step 2, freezes the encoder, and attempts to reconstruct images. This allows for real-time monitoring of the "World Model's" visual acuity (e.g., watching rumble strips appear in the reconstructions as the encoder learns).

```bash
python train_decoder.py
# Output: visuals/reconstruct_ep*.png

```

Heres an example, though features are still sharpening up:

<img src="reconstruct_ep43.png" width="800" alt="Reconstruction Epoch 43">

*Note: This is a test of the encoder to verify it is successfully extracting features and embedding them into a vector that can be reconstructed.*

### Step 4: Multi-Step Predictor Training

Trains the latent dynamics model. Unlike previous iterations, this uses a **Multi-Step Horizon (T+5)**. The model must predict a chain reaction of 5 future states. This forces it to learn causal physics rather than just copying the previous frame.

```bash
python train_predictor_multistep.py

```

### Step 5: Cost Model Training (The Judge)

Trains the energy-based discriminator. This model learns to distinguish "Expert" behavior (Low Energy) from "Random" behavior (High Energy) in the latent space. It serves as the navigation compass for the MPC.

```bash
python train_cost_model.py
# Output: models/cost_model_final.pth

```

## Component Architecture

### 1. The Encoder (`TinyEncoder`)

**Layman Explanation:**
The Encoder functions as the system's vision. It compresses raw, high-dimensional image data (pixels) into a compact summary vector. This vector allows the system to process the scene's content (e.g., "curve ahead") without processing every individual pixel.

**Technical Detail:**
The architecture is a modified **ResNet18** backbone. Max-pooling layers have been removed to preserve spatial granularity essential for the 64x64 input resolution. The training process uses a balanced loader to ensure the latent space encodes both track geometry and off-road textures.

### 2. The Multi-Step Predictor (`Predictor`)

**Layman Explanation:**
The Predictor functions as the system's internal simulator. It takes the current state summary and a proposed sequence of actions (e.g., "accelerate, then steer left") to calculate the likely outcome 5 steps into the future.

**Technical Detail:**
This is an autoregressive Multi-Layer Perceptron (MLP).

* **Input:** Current Latent State () and Action ().
* **Mechanism:** The output () is fed back into the input to predict , , etc.
* **Objective:** Minimize the Mean Squared Error (MSE) over the entire prediction horizon ( to ). This prevents "lazy" predictions where the model assumes the state is static.

### 3. The Decoder (`TinyDecoder`)

**Layman Explanation:**
The Decoder translates the internal numerical summaries back into visible images. It is primarily a debugging tool for humans to verify that the Encoder is actually "seeing" the road markers and grass boundaries correctly.

**Technical Detail:**
The architecture is a **Transposed Convolutional Network** (Inverse ResNet). It projects the 512-dimensional latent vector into a 4x4 spatial feature map and progressively upsamples it to the original 64x64 resolution using learned convolutional filters.

### 4. The Cost Model (`CostModel`)

**Layman Explanation:**
The Cost Model functions as the system's subconscious "gut feeling." When the Predictor imagines a future where the car is on the grass, the Cost Model assigns it a "High Energy" (Bad) score. When it imagines a future on the racing line, it assigns a "Low Energy" (Good) score.

**Technical Detail:**
A lightweight MLP Classifier trained on the frozen latent embeddings. It outputs a scalar probability score , where  represents the "Expert" distribution and  represents the "Random/Chaos" distribution.

## Energy-Based Autonomous Driving (MPC)

The final phase implements **Model Predictive Control (MPC)** using an Energy-Based Model (EBM) approach. Instead of targeting a specific geometric point, the car minimizes "Energy" in the latent landscape.

**Execution:**

```bash
python drive_mpc.py

```

**MPC Workflow:**

1. **Observation:** Encode the current camera frame into Latent State .
2. **Dreaming:** Generate 1,000 parallel action sequences using **Colored Noise** (temporally correlated random smoothing) to ensure drivable trajectories.
3. **Prediction:** The Predictor unrolls the future latent states () for all 1,000 futures simultaneously.
4. **Judgment:** The Cost Model scores every predicted future state.
* *Grass/Wall*  High Energy.
* *Track/Racing Line*  Low Energy.


5. **Action:** The system executes the first action of the sequence with the lowest cumulative energy.