# TinyAutoJEPA: Self-Supervised World Model Pilot

TinyAutoJEPA is an autonomous driving system based on Joint Embedding Predictive Architecture (JEPA). It focuses on learning the dynamics of an environment through self-supervised observation rather than direct imitation.

Unlike standard Imitation Learning, which clones human actions directly, this system builds a "World Model" (an internal representation of physics and causality). It learns to distinguish surface properties (e.g., grass vs. road) and predicts future states based on current actions before being tasked with driving.


## Dataset Strategy

The system relies on a massive, multi-modal dataset to learn a robust world model. The data is sourced from three distinct collection strategies to ensure a rich understanding of different driving conditions, preventing issues like "Catastrophic Forgetting" (e.g., forgetting grass physics because the car only drives on the track).

*   **Total Volume:** ~5.8 million frames.
*   **Generation:** Produced via 32 parallel CPU workers running different collection scripts.
*   **Composition:**
    *   **Expert Driving (~1.9M frames):** Generated by a pure-pursuit algorithm that follows racing lines perfectly. This teaches the model the ideal driving behavior ("Order").
    *   **Recovery Scenarios (~2.4M frames):** Generated by an expert policy that has intentional mistakes injected (e.g., understeering, oversteering, late braking). This forces the model to learn how to recover from errors and return to a stable state.
    *   **Random Exploration (~1.5M frames):** Generated by taking random actions to explore the full range of the environment, including off-road areas and unusual orientations ("Chaos").
*   **Balancing:** The data loader in the training scripts ensures a balanced mix of these data types in each batch, allowing the model to learn from all modalities simultaneously.

## Training Pipeline

### Step 1: Mass Data Generation

Generates trajectory data using the 32-thread collection scripts. This saturates the CPU to maximize throughput. The three scripts can be run in any order to generate the full dataset.

```bash
# Generate expert driving data
python collect_race_data.py
# Output: ./data_race/*.npz (~1.9M frames)

# Generate recovery scenario data
python collect_recovery_data.py
# Output: ./data_recovery/*.npz (~2.4M frames)

# Generate random exploration data
python collect_data.py
# Output: ./data/*.npz (~1.5M frames)
```

### Step 2: Encoder Training (The Foundation)

Trains the visual encoder on the primary GPU. This step uses a **VICReg** loss function to force the model to learn distinct features for different track elements without requiring labeled data.

```bash
python train_encoder.py
# Output: models/encoder_mixed_ep*.pth

```

### Step 3: Parallel Visual Verification

Runs simultaneously on the secondary GPU. This script hunts for new checkpoints generated by Step 2, freezes the encoder, and attempts to reconstruct images. This allows for real-time monitoring of the "World Model's" visual acuity (e.g., watching rumble strips appear in the reconstructions as the encoder learns).

```bash
python train_decoder.py
# Output: visuals/reconstruct_ep*.png

```

Heres an example, though features are still sharpening up:

<img src="reconstruct_ep43.png" width="800" alt="Reconstruction Epoch 43">

*Note: This is a test of the encoder to verify it is successfully extracting features and embedding them into a vector that can be reconstructed.*

### Step 4: Multi-Step Predictor Training

Trains the latent dynamics model. Unlike previous iterations, this uses a **Multi-Step Horizon (T+15)**. The model must predict a chain reaction of 15 future states. This forces it to learn causal physics rather than just copying the previous frame.

```bash
python train_predictor.py

```
Initial Dream visualisation, predictor generating possible futures: 

![Dream Visualization](output_dream.gif)

```bash
python train_cost_model.py
# Output: models/cost_model_final.pth

```

## Component Architecture

### 1. The Encoder (`TinyEncoder`)

**Layman Explanation:**
The Encoder functions as the system's vision. It compresses raw, high-dimensional image data (pixels) into a compact summary vector. This vector allows the system to process the scene's content (e.g., "curve ahead") without processing every individual pixel.

**Technical Detail:**
The architecture is a modified **ResNet18** backbone. Max-pooling layers have been removed to preserve spatial granularity essential for the 64x64 input resolution. The training process uses a balanced loader to ensure the latent space encodes both track geometry and off-road textures.

### 2. The Multi-Step Predictor (`Predictor`)

**Layman Explanation:**
The Predictor functions as the system's internal simulator. It takes the current state summary and a proposed sequence of actions (e.g., "accelerate, then steer left") to calculate the likely outcome 15 steps into the future.

**Technical Detail:**
This is an autoregressive Multi-Layer Perceptron (MLP).

* **Input:** Current Latent State () and Action ().
* **Mechanism:** The output () is fed back into the input to predict , , etc.
* **Objective:** Minimize the Mean Squared Error (MSE) over the entire prediction horizon ( to ). This prevents "lazy" predictions where the model assumes the state is static.

### 3. The Decoder (`TinyDecoder`)

**Layman Explanation:**
The Decoder translates the internal numerical summaries back into visible images. It is primarily a debugging tool for humans to verify that the Encoder is actually "seeing" the road markers and grass boundaries correctly.

**Technical Detail:**
The architecture is a **Transposed Convolutional Network** (Inverse ResNet). It projects the 512-dimensional latent vector into a 4x4 spatial feature map and progressively upsamples it to the original 64x64 resolution using learned convolutional filters.


**Execution:**

```bash
V_BASE=45 V_DROP=4 V_FLIP_DROP=8 V_MIN=14 \
RECORD=1 VIDEO_OUT=mpc_debug.mp4 \
OVERSPEED_DEADBAND=4.0 SPEED_BRAKE_K=0.02 \
OVERSPEED_GAS_CAP=0.90 SPEED_GAS_CAP=0.99 \
python drive_mpc.py

```

**MPC Workflow:**

### MPC Demo

![MPC Demo](run_mpc.gif)
